
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a53d10",
   "metadata": {
    "papermill": {
     "duration": 0.010143,
     "end_time": "2023-05-23T07:13:31.109599",
     "exception": false,
     "start_time": "2023-05-23T07:13:31.099456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8159660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:13:31.127800Z",
     "iopub.status.busy": "2023-05-23T07:13:31.127142Z",
     "iopub.status.idle": "2023-05-23T07:18:26.741495Z",
     "shell.execute_reply": "2023-05-23T07:18:26.740207Z"
    },
    "papermill": {
     "duration": 295.627418,
     "end_time": "2023-05-23T07:18:26.745224",
     "exception": false,
     "start_time": "2023-05-23T07:13:31.117806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pytorch_model-00001-of-00002.bin\r\n",
      "open_clip_pytorch_model.bin\r\n"
     ]
    }
   ],
   "source": [
    "pkgs = [\n",
    "    '/kaggle/input/diffusion2023-package/transformers-4.27.4-py3-none-any.whl',\n",
    "    '/kaggle/input/diffusion2023-package/ftfy-6.1.1-py3-none-any.whl',\n",
    "    '/kaggle/input/diffusion2023-package/timm-0.8.21.dev0-py3-none-any.whl',\n",
    "    '/kaggle/input/diffusion2023-package/open_clip_torch-2.19.0-py3-none-any.whl',\n",
    "]\n",
    "!pip install -qq --no-deps --no-python-version-warning --no-warn-conflicts --no-warn-script-location {' '.join(pkgs)} \n",
    "\n",
    "!mkdir -p pkgs\n",
    "!cp -r /kaggle/input/diffusion2023-package/sentence-transformers-2.2.2 ./pkgs\n",
    "!cd ./pkgs/sentence-transformers-2.2.2; pip install -qq .\n",
    "\n",
    "!cp -r /kaggle/input/diffusion2023-pretrained/blip2-opt-2.7b ./\n",
    "!cat /kaggle/working/blip2-opt-2.7b/pytorch_model-00001-of-00002.bin.tar.gz* > /kaggle/working/blip2-opt-2.7b/pytorch_model-00001-of-00002.bin.tar.gz\n",
    "!cd /kaggle/working/blip2-opt-2.7b/; tar -xzvf pytorch_model-00001-of-00002.bin.tar.gz\n",
    "!rm -rf /kaggle/working/blip2-opt-2.7b/pytorch_model-00001-of-00002.bin.tar.gz*\n",
    "\n",
    "!cp -r /kaggle/input/diffusion2023-pretrained/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup ./\n",
    "!cd /kaggle/working/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup/;cat open_clip_pytorch_model.bin.tar* | tar -xv\n",
    "!rm -rf /kaggle/working/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup/open_clip_pytorch_model.bin.tar*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5213d6a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:18:26.765611Z",
     "iopub.status.busy": "2023-05-23T07:18:26.764655Z",
     "iopub.status.idle": "2023-05-23T07:18:30.934551Z",
     "shell.execute_reply": "2023-05-23T07:18:30.933280Z"
    },
    "papermill": {
     "duration": 4.182034,
     "end_time": "2023-05-23T07:18:30.937083",
     "exception": false,
     "start_time": "2023-05-23T07:18:26.755049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p raw\n",
    "!ln -s /kaggle/input/stable-diffusion-image-to-prompts/* ./raw/\n",
    "\n",
    "!mkdir -p ./result/models/\n",
    "!ln -s /kaggle/input/diffusion2023-model/* ./result/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1232d803",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:18:30.956556Z",
     "iopub.status.busy": "2023-05-23T07:18:30.955686Z",
     "iopub.status.idle": "2023-05-23T07:18:43.978778Z",
     "shell.execute_reply": "2023-05-23T07:18:43.977847Z"
    },
    "papermill": {
     "duration": 13.03495,
     "end_time": "2023-05-23T07:18:43.981141",
     "exception": false,
     "start_time": "2023-05-23T07:18:30.946191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import gc\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import open_clip\n",
    "\n",
    "from tqdm import tqdm\n",
    "from argparse import Namespace\n",
    "from PIL import Image\n",
    "from scipy import spatial\n",
    "from typing import Optional, Tuple\n",
    "from multiprocessing import Process\n",
    "from torch.nn import DataParallel\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "from transformers import CLIPProcessor\n",
    "from transformers import CLIPModel\n",
    "from transformers.models.clip import CLIPConfig\n",
    "from transformers import Blip2ForConditionalGeneration\n",
    "\n",
    "opj = os.path.join\n",
    "ope = os.path.exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09c4d6",
   "metadata": {
    "papermill": {
     "duration": 0.008012,
     "end_time": "2023-05-23T07:18:43.997716",
     "exception": false,
     "start_time": "2023-05-23T07:18:43.989704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0bb1bdc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-23T07:18:44.015281Z",
     "iopub.status.busy": "2023-05-23T07:18:44.014983Z",
     "iopub.status.idle": "2023-05-23T07:18:44.046889Z",
     "shell.execute_reply": "2023-05-23T07:18:44.046076Z"
    },
    "papermill": {
     "duration": 0.043366,
     "end_time": "2023-05-23T07:18:44.049216",
     "exception": false,
     "start_time": "2023-05-23T07:18:44.005850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('/kaggle/input/stable-diffusion-image-to-prompts/sample_submission.csv')\n",
    "IS_KAGGLE_SUBMIT = len(sample_submission) != 7 * 384\n",
    "RESULT_DIR = '/kaggle/working/result'\n",
    "DATA_DIR = '/kaggle/working'\n",
    "TRANSFORMERS = {\n",
    "    'openai/clip-vit-large-patch14-336': '/kaggle/input/diffusion2023-pretrained/clip-vit-large-patch14-336',\n",
    "    'Salesforce/blip2-opt-2.7b': '/kaggle/working/blip2-opt-2.7b',\n",
    "    'laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup': \"/kaggle/working/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup\",\n",
    "}\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = lambda model_name: TRANSFORMERS.get(model_name, model_name)\n",
    "\n",
    "EMBEDDING_LENGTH = 384\n",
    "ID = 'imgId_eId'\n",
    "TARGET = 'val'\n",
    "IMG_ID = 'imgId'\n",
    "PROMPT = 'prompt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a38fc",
   "metadata": {
    "papermill": {
     "duration": 0.007883,
     "end_time": "2023-05-23T07:18:44.065497",
     "exception": false,
     "start_time": "2023-05-23T07:18:44.057614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee7e86f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:18:44.082909Z",
     "iopub.status.busy": "2023-05-23T07:18:44.082636Z",
     "iopub.status.idle": "2023-05-23T07:18:44.091434Z",
     "shell.execute_reply": "2023-05-23T07:18:44.090574Z"
    },
    "papermill": {
     "duration": 0.019702,
     "end_time": "2023-05-23T07:18:44.093325",
     "exception": false,
     "start_time": "2023-05-23T07:18:44.073623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DiffusionDataset(Dataset):\n",
    "    def __init__(self, args, dataset='valid', transform=None):\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.df, self.targets = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.img_dir = f'{DATA_DIR}/raw/images'\n",
    "        test_df = pd.read_csv(f'{DATA_DIR}/raw/sample_submission.csv')\n",
    "        test_df[[IMG_ID, 'eId']] = test_df['imgId_eId'].str.split(\"_\", expand=True)\n",
    "        df = test_df[[IMG_ID]].drop_duplicates().reset_index(drop=True)\n",
    "        targets = test_df['val'].values.reshape(-1, EMBEDDING_LENGTH)\n",
    "        df[PROMPT] = ''\n",
    "        return df, targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        img_id = row[IMG_ID]\n",
    "        img_file = f'{self.img_dir}/{img_id}.png'\n",
    "        image = Image.open(img_file)\n",
    "        inputs = self.transform(image)\n",
    "        inputs['target'] = torch.tensor(self.targets[index])\n",
    "        return inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69a8c1",
   "metadata": {
    "papermill": {
     "duration": 0.007849,
     "end_time": "2023-05-23T07:18:44.109364",
     "exception": false,
     "start_time": "2023-05-23T07:18:44.101515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd7316c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:18:44.127370Z",
     "iopub.status.busy": "2023-05-23T07:18:44.126955Z",
     "iopub.status.idle": "2023-05-23T07:18:44.138163Z",
     "shell.execute_reply": "2023-05-23T07:18:44.137325Z"
    },
    "papermill": {
     "duration": 0.022579,
     "end_time": "2023-05-23T07:18:44.140294",
     "exception": false,
     "start_time": "2023-05-23T07:18:44.117715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def state_dict_replace(load_state_dict, replace_key='transformer.'):\n",
    "    new_load_state_dict = dict()\n",
    "    for key in load_state_dict.keys():\n",
    "        if key.count(replace_key) > 0:\n",
    "            dst_key = key.replace(replace_key, '')\n",
    "        else:\n",
    "            dst_key = key\n",
    "        new_load_state_dict[dst_key] = load_state_dict[key]\n",
    "    load_state_dict = new_load_state_dict\n",
    "    return load_state_dict\n",
    "\n",
    "\n",
    "def load_pretrained(net, pretrained_file, strict=False, can_print=True):\n",
    "    if can_print:\n",
    "        print(f'load pretrained file: {pretrained_file}')\n",
    "    load_state_dict = torch.load(pretrained_file, map_location=torch.device('cpu'))\n",
    "    net = load_pretrained_state_dict(net, load_state_dict, strict=strict, can_print=can_print)\n",
    "    return net\n",
    "\n",
    "\n",
    "def load_pretrained_state_dict(net, load_state_dict, strict=False, can_print=True):\n",
    "    if 'epoch' in load_state_dict and can_print:\n",
    "        epoch = load_state_dict['epoch']\n",
    "        print(f'load epoch:{epoch:.2f}')\n",
    "    if 'state_dict' in load_state_dict:\n",
    "        load_state_dict = load_state_dict['state_dict']\n",
    "    if type(net) == DataParallel or type(net) == DistributedDataParallel:\n",
    "        state_dict = net.module.state_dict()\n",
    "    else:\n",
    "        state_dict = net.state_dict()\n",
    "    load_state_dict = state_dict_replace(load_state_dict, '_orig_mod.')\n",
    "    for key in list(load_state_dict.keys()):\n",
    "        if key not in state_dict:\n",
    "            if strict:\n",
    "                raise Exception(f'not in {key}')\n",
    "            if can_print:\n",
    "                print('not in', key)\n",
    "            continue\n",
    "        if load_state_dict[key].size() != state_dict[key].size():\n",
    "            if strict:\n",
    "                raise Exception(f'size not the same {key}')\n",
    "            if can_print:\n",
    "                print('size not the same', key)\n",
    "            continue\n",
    "        state_dict[key] = load_state_dict[key]\n",
    "    if type(net) == DataParallel or type(net) == DistributedDataParallel:\n",
    "        net.module.load_state_dict(state_dict)\n",
    "    else:\n",
    "        net.load_state_dict(state_dict)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd95242",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:18:44.157854Z",
     "iopub.status.busy": "2023-05-23T07:18:44.157138Z",
     "iopub.status.idle": "2023-05-23T07:18:44.167121Z",
     "shell.execute_reply": "2023-05-23T07:18:44.166341Z"
    },
    "papermill": {
     "duration": 0.020918,
     "end_time": "2023-05-23T07:18:44.169215",
     "exception": false,
     "start_time": "2023-05-23T07:18:44.148297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Blip2ValidTransformer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        from transformers import AutoProcessor\n",
    "        self.blip2_processor = AutoProcessor.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH(F\"Salesforce/{args.model_name}\"))\n",
    "        self.blip2_processor.image_processor.size = {'height': self.args.image_size, 'width': self.args.image_size}\n",
    "\n",
    "    def __call__(self, image, text=None, return_tensors=\"pt\", truncation=True, max_length=None, padding='max_length'):\n",
    "        data = self.blip2_processor(\n",
    "            image,\n",
    "            text=text,\n",
    "            return_tensors=return_tensors,\n",
    "            truncation=truncation,\n",
    "            max_length=max_length,\n",
    "            padding=padding\n",
    "        )\n",
    "        for key in data.keys():\n",
    "            data[key] = data[key].squeeze(0)\n",
    "        return data\n",
    "\n",
    "\n",
    "class CLIPValidTransformer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH(args.model_name))\n",
    "        self.clip_processor.image_processor.size['shortest_edge'] = args.image_size\n",
    "        self.clip_processor.image_processor.do_center_crop = False\n",
    "\n",
    "    def __call__(self, image):\n",
    "        data = self.clip_processor(images=image)['pixel_values'][0]\n",
    "        return {\n",
    "            'image': data\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6232af63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:18:44.187216Z",
     "iopub.status.busy": "2023-05-23T07:18:44.186955Z",
     "iopub.status.idle": "2023-05-23T07:18:44.216407Z",
     "shell.execute_reply": "2023-05-23T07:18:44.215453Z"
    },
    "papermill": {
     "duration": 0.041046,
     "end_time": "2023-05-23T07:18:44.218747",
     "exception": false,
     "start_time": "2023-05-23T07:18:44.177701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.device = 'cuda'\n",
    "        self.submit_dir = f'{RESULT_DIR}/submissions/{args.out_dir}/fold{args.fold}'\n",
    "        os.makedirs(self.submit_dir, exist_ok=True)\n",
    "\n",
    "    def get_dataloader(self):\n",
    "        args = self.args\n",
    "        # set transform\n",
    "        if args.is_clip_model:\n",
    "            transform = CLIPValidTransformer(args)\n",
    "        elif args.model_name.startswith('blip'):\n",
    "            transform = Blip2ValidTransformer(args)\n",
    "        # create dataset, dataloader\n",
    "        test_dataset = DiffusionDataset(args, dataset=args.dataset, transform=transform)\n",
    "        test_sampler = SequentialSampler(test_dataset)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            sampler=test_sampler,\n",
    "            drop_last=False,\n",
    "            pin_memory=True,\n",
    "            batch_size=args.test_batch_size,\n",
    "            num_workers=args.num_workers,\n",
    "        )\n",
    "        return test_dataloader\n",
    "\n",
    "    def load_model(self, epoch):\n",
    "        args = self.args\n",
    "        model = eval(args.model)(args)\n",
    "        model_path = f'{RESULT_DIR}/models/{args.out_dir}/fold{args.fold}/{epoch}.pth'\n",
    "        load_pretrained(model, model_path, strict=False, can_print=True)\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def do_predict(self, dataloader, model):\n",
    "        model.eval()\n",
    "        tbar = tqdm(dataloader, file=sys.stdout)\n",
    "        N = len(dataloader.dataset)\n",
    "        print(f'dataset len: {N}')\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                for idx, data in enumerate(tbar):\n",
    "                    logits = get_outputs(data, model)\n",
    "                    preds.append(logits.detach().cpu().numpy())\n",
    "        prompt_embeddings = np.vstack(preds)\n",
    "        imgIds = dataloader.dataset.df[IMG_ID].values\n",
    "        return prompt_embeddings, imgIds\n",
    "\n",
    "    def save_embeddings(self, embeddings, ids):\n",
    "        file = F\"{self.submit_dir}/embeddings_{self.args.dataset}_epoch{self.args.epoch}.npz\"\n",
    "        np.savez_compressed(file, embeddings=embeddings, ids=ids)\n",
    "\n",
    "\n",
    "def make_parser():\n",
    "    parser = argparse.ArgumentParser(description='generate_submission')\n",
    "    parser.add_argument('-f', type=str, default=None)\n",
    "    parser.add_argument('--HistoryManager.hist_file', type=str, default='')\n",
    "    # model config\n",
    "    parser.add_argument('--module', '-m', type=str, default='basic_net', help='module')\n",
    "    parser.add_argument('--model', type=str, default=None, help='model')\n",
    "    parser.add_argument('--model_name', type=str, default='vit_base_patch16_224', help='model_name')\n",
    "    parser.add_argument('--embedding_dim', default=384, type=int)\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=None, required=False)\n",
    "    parser.add_argument(\"--epoch\", type=str, default='best_ema', required=False)  \n",
    "    # dataset config\n",
    "    parser.add_argument('--dataset', default='valid', type=str)  \n",
    "    parser.add_argument('--split_type', default='random', type=str)\n",
    "    parser.add_argument('--folds_num', default=1, type=int)\n",
    "    parser.add_argument('--fold', default=0, type=int)\n",
    "    parser.add_argument('--image_size', default=224, type=int)\n",
    "    # predict param\n",
    "    parser.add_argument('--test_batch_size', default=16, type=int)\n",
    "    parser.add_argument(\"--gpus\", type=str, default=\"0\", required=False)\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "    parser.add_argument('--save_embeddings', default=1, type=int, help='') \n",
    "    parser.add_argument('--update', default=1, type=int, help='') \n",
    "    return parser\n",
    "\n",
    "\n",
    "def get_outputs(data, model):\n",
    "    keys = ['image', 'pixel_values', 'target', 'input_ids', 'attention_mask']\n",
    "    for key in keys:\n",
    "        if key in data:\n",
    "            data[key] = Variable(data[key].cuda())\n",
    "    return model(data)\n",
    "\n",
    "\n",
    "def generate_submission(embeddings, ids):\n",
    "    imgId_eId = [\n",
    "        '_'.join(map(str, i)) for i in zip(\n",
    "            np.repeat(ids, EMBEDDING_LENGTH),\n",
    "            np.tile(range(EMBEDDING_LENGTH), len(ids))\n",
    "        )\n",
    "    ]\n",
    "    submission = pd.DataFrame({\n",
    "        'imgId_eId': imgId_eId,\n",
    "        'val': embeddings.flatten(),\n",
    "    })\n",
    "    return submission\n",
    "\n",
    "\n",
    "def cosine_similarity(y_trues, y_preds):\n",
    "    scores = [\n",
    "        1 - spatial.distance.cosine(y_true, y_pred)\n",
    "        for y_true, y_pred in zip(y_trues, y_preds)\n",
    "    ]\n",
    "    return np.mean(scores)\n",
    "        \n",
    "def _do_predict(params):\n",
    "    parser = make_parser()\n",
    "    args = parser.parse_args(namespace=Namespace(**params))\n",
    "    args.can_print = True\n",
    "    args.is_clip_model = args.model_name.startswith('openai/clip') or args.model_name.startswith('laion/CLIP')\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpus\n",
    "\n",
    "    predictor = Predictor(args)\n",
    "    pred_path_suffix = f'{args.dataset}_epoch{args.epoch}'\n",
    "    pred_path = f'{predictor.submit_dir}/{pred_path_suffix}_pred.csv'\n",
    "\n",
    "    dataloader = predictor.get_dataloader()\n",
    "    if not ope(pred_path) or args.update == 1:\n",
    "        model = predictor.load_model(args.epoch)\n",
    "        if len(args.gpus.split(',')) > 1:\n",
    "            model = DataParallel(model)\n",
    "        embeddings, ids = predictor.do_predict(dataloader, model)\n",
    "        pred_df = generate_submission(embeddings, ids)\n",
    "        if args.save_embeddings:\n",
    "            predictor.save_embeddings(embeddings, ids)\n",
    "        pred_df.to_csv(pred_path, index=False)\n",
    "        print(f'save pred to: {pred_path}')\n",
    "    else:\n",
    "        pred_df = pd.read_csv(pred_path)\n",
    "        print(f'load pred from: {pred_path}')\n",
    "    print(pred_df.head())\n",
    "\n",
    "    check_df = pred_df['imgId_eId'].str.split(\"_\", expand=True)\n",
    "    check_df.columns = [IMG_ID, 'eId']\n",
    "    indexs = check_df['eId'].values.reshape(-1, EMBEDDING_LENGTH).astype(np.int16)\n",
    "    assert np.all(indexs == range(EMBEDDING_LENGTH))\n",
    "\n",
    "    if not IS_KAGGLE_SUBMIT:\n",
    "        pred_embeddings = pred_df['val'].values.reshape(-1, EMBEDDING_LENGTH)\n",
    "        pred_ids = check_df[IMG_ID].drop_duplicates().values\n",
    "        truth_embeddings = dataloader.dataset.targets\n",
    "        truth_ids = dataloader.dataset.df[IMG_ID].values\n",
    "        assert np.all(truth_ids == pred_ids)\n",
    "        score = cosine_similarity(truth_embeddings, pred_embeddings)\n",
    "        print(f'score: {score:.5f}')\n",
    "\n",
    "def do_predict(params):\n",
    "    p = Process(target=_do_predict, args=(params,))\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f81578c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:18:44.236326Z",
     "iopub.status.busy": "2023-05-23T07:18:44.235608Z",
     "iopub.status.idle": "2023-05-23T07:18:44.245438Z",
     "shell.execute_reply": "2023-05-23T07:18:44.244659Z"
    },
    "papermill": {
     "duration": 0.02062,
     "end_time": "2023-05-23T07:18:44.247371",
     "exception": false,
     "start_time": "2023-05-23T07:18:44.226751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LoRALinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=4):\n",
    "        super().__init__()\n",
    "\n",
    "        if rank > min(in_features, out_features):\n",
    "            raise ValueError(f\"LoRA rank {rank} must be less or equal than {min(in_features, out_features)}\")\n",
    "\n",
    "        self.down = nn.Linear(in_features, rank, bias=False)\n",
    "        self.up = nn.Linear(rank, out_features, bias=False)\n",
    "\n",
    "        nn.init.normal_(self.down.weight, std=1 / rank)\n",
    "        nn.init.zeros_(self.up.weight)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        orig_dtype = hidden_states.dtype\n",
    "        dtype = self.down.weight.dtype\n",
    "\n",
    "        down_hidden_states = self.down(hidden_states.to(dtype))\n",
    "        up_hidden_states = self.up(down_hidden_states)\n",
    "\n",
    "        return up_hidden_states.to(orig_dtype)\n",
    "\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, attn, rank=4, lora_scale=1):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.rank = rank\n",
    "        self.lora_scale = lora_scale\n",
    "\n",
    "        in_features = attn.in_features\n",
    "        out_features = attn.out_features\n",
    "        _rank = min(in_features // rank, out_features // rank)\n",
    "        self.fc_lora = LoRALinearLayer(in_features, out_features, _rank)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.attn(hidden_states) + self.lora_scale * self.fc_lora(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59042f65",
   "metadata": {
    "papermill": {
     "duration": 0.007848,
     "end_time": "2023-05-23T07:18:44.263385",
     "exception": false,
     "start_time": "2023-05-23T07:18:44.255537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BLIP-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a63af9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:18:44.280769Z",
     "iopub.status.busy": "2023-05-23T07:18:44.280523Z",
     "iopub.status.idle": "2023-05-23T07:18:44.292337Z",
     "shell.execute_reply": "2023-05-23T07:18:44.291531Z"
    },
    "papermill": {
     "duration": 0.022775,
     "end_time": "2023-05-23T07:18:44.294249",
     "exception": false,
     "start_time": "2023-05-23T07:18:44.271474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Blip2LoRAAttnProcessor(nn.Module):\n",
    "    def __init__(self, attn, rank=4, lora_scale=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = attn\n",
    "        config = attn.config\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        # small tweak here compared to CLIP, no bias here\n",
    "        self.qkv_lora = LoRALinearLayer(embed_dim, 3 * embed_dim, embed_dim // rank)\n",
    "        self.projection_lora = LoRALinearLayer(embed_dim, embed_dim, embed_dim // rank)\n",
    "        self.rank = rank\n",
    "        self.lora_scale = lora_scale\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states: torch.Tensor,\n",
    "            head_mask: Optional[torch.Tensor] = None,\n",
    "            output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
    "\n",
    "        mixed_qkv = self.attn.qkv(hidden_states) + self.lora_scale * self.qkv_lora(hidden_states)\n",
    "\n",
    "        mixed_qkv = mixed_qkv.reshape(bsz, tgt_len, 3, self.attn.num_heads, embed_dim // self.attn.num_heads).permute(\n",
    "            2, 0, 3, 1, 4\n",
    "        )\n",
    "        query_states, key_states, value_states = (\n",
    "            mixed_qkv[0],\n",
    "            mixed_qkv[1],\n",
    "            mixed_qkv[2],\n",
    "        )\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_states, key_states.transpose(-1, -2))\n",
    "\n",
    "        attention_scores = attention_scores * self.attn.scale\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.attn.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_states).permute(0, 2, 1, 3)\n",
    "\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.attn.embed_dim,)\n",
    "        context_layer = context_layer.reshape(new_context_layer_shape)\n",
    "\n",
    "        output = self.attn.projection(context_layer) + self.lora_scale * self.projection_lora(context_layer)\n",
    "\n",
    "        outputs = (output, attention_probs) if output_attentions else (output, None)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baafd06b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:18:44.311814Z",
     "iopub.status.busy": "2023-05-23T07:18:44.311139Z",
     "iopub.status.idle": "2023-05-23T07:18:44.318057Z",
     "shell.execute_reply": "2023-05-23T07:18:44.317271Z"
    },
    "papermill": {
     "duration": 0.017555,
     "end_time": "2023-05-23T07:18:44.319948",
     "exception": false,
     "start_time": "2023-05-23T07:18:44.302393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lora_blip2_model(model, lora_scale, lora_rate):\n",
    "    layer_num = len(model.vision_model.encoder.layers)\n",
    "    lora_num = int(layer_num * lora_rate)\n",
    "    for _i in range(layer_num - lora_num, layer_num):\n",
    "        attn = model.vision_model.encoder.layers[_i].self_attn\n",
    "        new_attn = Blip2LoRAAttnProcessor(attn, rank=4, lora_scale=lora_scale)\n",
    "        model.vision_model.encoder.layers[_i].self_attn = new_attn\n",
    "        model.vision_model.encoder.layers[_i].mlp.fc1 = LoRALinear(\n",
    "            model.vision_model.encoder.layers[_i].mlp.fc1, rank=4, lora_scale=lora_scale)\n",
    "        model.vision_model.encoder.layers[_i].mlp.fc2 = LoRALinear(\n",
    "            model.vision_model.encoder.layers[_i].mlp.fc2, rank=4, lora_scale=lora_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d98b9b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:18:44.337383Z",
     "iopub.status.busy": "2023-05-23T07:18:44.336675Z",
     "iopub.status.idle": "2023-05-23T07:18:44.346704Z",
     "shell.execute_reply": "2023-05-23T07:18:44.345945Z"
    },
    "papermill": {
     "duration": 0.020671,
     "end_time": "2023-05-23T07:18:44.348630",
     "exception": false,
     "start_time": "2023-05-23T07:18:44.327959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class BasicNet(torch.nn.Module):\n",
    "    def __init__(self, args, lora_scale=1, lora_rate=0.4):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "            TRANSFORMERS.get(F\"Salesforce/{args.model_name}\"),\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        del self.model.language_model\n",
    "        del self.model.language_projection\n",
    "        self.model.vision_model.requires_grad_(False)\n",
    "        self.model.feat_proj = torch.nn.Linear(self.model.qformer.config.hidden_size * 32, self.args.embedding_dim)\n",
    "        lora_blip2_model(self.model, lora_scale, lora_rate)\n",
    "\n",
    "    def forward(self, samples):\n",
    "        pixel_values = samples['pixel_values']\n",
    "        pixel_values = pixel_values.half()\n",
    "        image_embeds = self.model.vision_model(pixel_values, return_dict=True).last_hidden_state\n",
    "        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n",
    "\n",
    "        query_tokens = self.model.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "        query_outputs = self.model.qformer(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=image_embeds,\n",
    "            encoder_attention_mask=image_attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        query_output = query_outputs.last_hidden_state\n",
    "\n",
    "        feat_pred = self.model.feat_proj(query_output.view(len(query_output), -1))\n",
    "        return feat_pred\n",
    "\n",
    "\n",
    "def blip2_model_lora04(args):\n",
    "    model = BasicNet(args, lora_scale=1, lora_rate=0.4)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dfe7739",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:18:44.366223Z",
     "iopub.status.busy": "2023-05-23T07:18:44.365442Z",
     "iopub.status.idle": "2023-05-23T07:19:42.639145Z",
     "shell.execute_reply": "2023-05-23T07:19:42.637750Z"
    },
    "papermill": {
     "duration": 58.286257,
     "end_time": "2023-05-23T07:19:42.642962",
     "exception": false,
     "start_time": "2023-05-23T07:18:44.356705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained file: /kaggle/working/result/models/train2m_pretrain6-6m_lora04_blip2_opt_2_7b_224x224_1folds/fold0/en3_ema.pth\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]dataset len: 7\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.38s/it]\n",
      "save pred to: /kaggle/working/result/submissions/train2m_pretrain6-6m_lora04_blip2_opt_2_7b_224x224_1folds/fold0/test_epochen3_ema_pred.csv\n",
      "     imgId_eId       val\n",
      "0  20057f34d_0  4.132812\n",
      "1  20057f34d_1  6.898438\n",
      "2  20057f34d_2  1.640625\n",
      "3  20057f34d_3  0.455566\n",
      "4  20057f34d_4 -0.048065\n",
      "score: 0.68943\n"
     ]
    }
   ],
   "source": [
    "do_predict({\n",
    "    'module': 'blip2_net', \n",
    "    'model': 'blip2_model_lora04', \n",
    "    'model_name': 'blip2-opt-2.7b',  \n",
    "    'out_dir': 'train2m_pretrain6-6m_lora04_blip2_opt_2_7b_224x224_1folds', \n",
    "    'epoch': 'en3_ema', \n",
    "    'fold': 0, \n",
    "    'test_batch_size': 256, \n",
    "    'dataset': 'test', \n",
    "    'gpus': '0',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511258bb",
   "metadata": {
    "papermill": {
     "duration": 0.009109,
     "end_time": "2023-05-23T07:19:42.662403",
     "exception": false,
     "start_time": "2023-05-23T07:19:42.653294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ConvNeXt-XXLarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b4d0a31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:19:42.683509Z",
     "iopub.status.busy": "2023-05-23T07:19:42.682632Z",
     "iopub.status.idle": "2023-05-23T07:19:42.692671Z",
     "shell.execute_reply": "2023-05-23T07:19:42.691673Z"
    },
    "papermill": {
     "duration": 0.023321,
     "end_time": "2023-05-23T07:19:42.694750",
     "exception": false,
     "start_time": "2023-05-23T07:19:42.671429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lora_convnext_model(model, args, lora_scale, lora_rate, nfz_rate=0.4, rank=4):\n",
    "    model.trunk.stem.requires_grad_(False)\n",
    "    model.trunk.stages[0].requires_grad_(False)\n",
    "    model.trunk.stages[1].requires_grad_(False)\n",
    "    model.trunk.stages[2].requires_grad_(False)\n",
    "    layer_num = len(model.trunk.stages[2].blocks)\n",
    "    lora_num = int(layer_num * lora_rate)\n",
    "    nfz_num = int(layer_num * nfz_rate)\n",
    "\n",
    "    for _idx in range(layer_num - lora_num - nfz_num, layer_num):\n",
    "        if (layer_num-_idx) <= nfz_num:\n",
    "            model.trunk.stages[2].blocks[_idx].requires_grad_(True)\n",
    "            continue\n",
    "        model.trunk.stages[2].blocks[_idx].mlp.fc1 = LoRALinear(model.trunk.stages[2].blocks[_idx].mlp.fc1, rank=rank, lora_scale=lora_scale)\n",
    "        model.trunk.stages[2].blocks[_idx].mlp.fc2 = LoRALinear(model.trunk.stages[2].blocks[_idx].mlp.fc2, rank=rank, lora_scale=lora_scale)\n",
    "    if args.can_print: print(f'lora clip_model, scale:{lora_scale} rate:{lora_rate} num:{lora_num}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fca0d8f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:19:42.714429Z",
     "iopub.status.busy": "2023-05-23T07:19:42.714141Z",
     "iopub.status.idle": "2023-05-23T07:19:42.722616Z",
     "shell.execute_reply": "2023-05-23T07:19:42.721773Z"
    },
    "papermill": {
     "duration": 0.020468,
     "end_time": "2023-05-23T07:19:42.724559",
     "exception": false,
     "start_time": "2023-05-23T07:19:42.704091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseClipNet(torch.nn.Module):\n",
    "    def __init__(self, args, lora_rate=0.0, nfz_rate=0.0, lora_scale=1.0, rank=4, model_type='ViT-L-14', ebd_dim=768):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(model_type,\n",
    "                                                                     pretrained=f'{PRETRAINED_MODEL_NAME_OR_PATH(args.model_name)}/open_clip_pytorch_model.bin')\n",
    "        self.vision_model = model.visual\n",
    "        fc_dim = 16 * 1024\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(ebd_dim, fc_dim),\n",
    "            nn.BatchNorm1d(fc_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_dim, args.embedding_dim),\n",
    "        )\n",
    "        lora_convnext_model(self.vision_model, args, lora_scale, lora_rate, nfz_rate, rank)\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = self.vision_model(data['image'])\n",
    "        logits = self.head(out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def clipnet_convnext_xxlarge_256_lora04_nfz02_rank8(args):\n",
    "    model = BaseClipNet(args, lora_rate=0.4, nfz_rate=0.2, model_type='convnext_xxlarge', rank=8, ebd_dim=1024)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8c8cffc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:19:42.744441Z",
     "iopub.status.busy": "2023-05-23T07:19:42.743709Z",
     "iopub.status.idle": "2023-05-23T07:20:55.460072Z",
     "shell.execute_reply": "2023-05-23T07:20:55.458525Z"
    },
    "papermill": {
     "duration": 72.730258,
     "end_time": "2023-05-23T07:20:55.463832",
     "exception": false,
     "start_time": "2023-05-23T07:19:42.733574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora clip_model, scale:1.0 rate:0.4 num:12\n",
      "load pretrained file: /kaggle/working/result/models/train2m_pretrain6-6m_lora04_laion_CLIP_convnext_xxlarge_laion2B_s34B_b82K_augreg_soup_256x256_1folds/fold0/en3_ema.pth\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]dataset len: 7\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.04s/it]\n",
      "save pred to: /kaggle/working/result/submissions/train2m_pretrain6-6m_lora04_laion_CLIP_convnext_xxlarge_laion2B_s34B_b82K_augreg_soup_256x256_1folds/fold0/test_epochen3_ema_pred.csv\n",
      "     imgId_eId        val\n",
      "0  20057f34d_0   6.277344\n",
      "1  20057f34d_1  20.250000\n",
      "2  20057f34d_2   2.728516\n",
      "3  20057f34d_3  -0.554199\n",
      "4  20057f34d_4  -2.425781\n",
      "score: 0.68653\n"
     ]
    }
   ],
   "source": [
    "do_predict({\n",
    "    'out_dir': 'train2m_pretrain6-6m_lora04_laion_CLIP_convnext_xxlarge_laion2B_s34B_b82K_augreg_soup_256x256_1folds',\n",
    "    'module': 'clip_convnext_net',\n",
    "    'model': 'clipnet_convnext_xxlarge_256_lora04_nfz02_rank8',\n",
    "    'model_name': 'laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup',\n",
    "    'image_size': 256,\n",
    "    'gpus': '0',\n",
    "    'epoch': 'en3_ema',\n",
    "    'dataset': 'test',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06252655",
   "metadata": {
    "papermill": {
     "duration": 0.012648,
     "end_time": "2023-05-23T07:20:55.489037",
     "exception": false,
     "start_time": "2023-05-23T07:20:55.476389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ViT-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09bed822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:20:55.516485Z",
     "iopub.status.busy": "2023-05-23T07:20:55.516120Z",
     "iopub.status.idle": "2023-05-23T07:20:55.542434Z",
     "shell.execute_reply": "2023-05-23T07:20:55.541601Z"
    },
    "papermill": {
     "duration": 0.043198,
     "end_time": "2023-05-23T07:20:55.544368",
     "exception": false,
     "start_time": "2023-05-23T07:20:55.501170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LoRACLIPAttention(nn.Module):\n",
    "    def __init__(self, attn, rank=4, lora_scale=1):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.rank = rank\n",
    "        self.lora_scale = lora_scale\n",
    "\n",
    "        config = attn.config\n",
    "        embed_dim = config.hidden_size\n",
    "        self.k_proj_lora = LoRALinearLayer(embed_dim, embed_dim, embed_dim // rank)\n",
    "        self.v_proj_lora = LoRALinearLayer(embed_dim, embed_dim, embed_dim // rank)\n",
    "        self.q_proj_lora = LoRALinearLayer(embed_dim, embed_dim, embed_dim // rank)\n",
    "        self.out_proj_lora = LoRALinearLayer(embed_dim, embed_dim, embed_dim // rank)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states: torch.Tensor,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "            output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = (self.attn.q_proj(hidden_states) + self.lora_scale * self.q_proj_lora(\n",
    "            hidden_states)) * self.attn.scale\n",
    "        key_states = self.attn._shape(\n",
    "            self.attn.k_proj(hidden_states) + self.lora_scale * self.k_proj_lora(hidden_states), -1, bsz)\n",
    "        value_states = self.attn._shape(\n",
    "            self.attn.v_proj(hidden_states) + self.lora_scale * self.v_proj_lora(hidden_states), -1, bsz)\n",
    "\n",
    "        proj_shape = (bsz * self.attn.num_heads, -1, self.attn.head_dim)\n",
    "        query_states = self.attn._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.view(*proj_shape)\n",
    "        value_states = value_states.view(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.attn.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.attn.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        # apply the causal_attention_mask first\n",
    "        if causal_attention_mask is not None:\n",
    "            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n",
    "                    f\" {causal_attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.attn.num_heads, tgt_len, src_len) + causal_attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.attn.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.attn.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = attn_weights.view(bsz * self.attn.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit akward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.attn.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.attn.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.attn.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.attn.num_heads, tgt_len, self.attn.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.attn.num_heads, tgt_len, self.attn.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.attn.num_heads, tgt_len, self.attn.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
    "\n",
    "        attn_output = self.attn.out_proj(attn_output) + self.lora_scale * self.out_proj_lora(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped\n",
    "    \n",
    "def lora_clip_model(model, args, lora_scale, lora_rate, nfz_rate=0.4, rank=4):\n",
    "    model.embeddings.requires_grad_(False)\n",
    "    model.pre_layrnorm.requires_grad_(False)\n",
    "    model.encoder.layers.requires_grad_(False)\n",
    "    layer_num = len(model.encoder.layers)\n",
    "    lora_num = int(layer_num * lora_rate)\n",
    "    nfz_num = int(layer_num * nfz_rate)\n",
    "    for _i in range(layer_num - lora_num - nfz_num, layer_num):\n",
    "        if (layer_num - _i) <= nfz_num:\n",
    "            model.encoder.layers[_i].requires_grad_(True)\n",
    "            continue\n",
    "        attn = model.encoder.layers[_i].self_attn\n",
    "        new_attn = LoRACLIPAttention(attn, rank=rank, lora_scale=lora_scale)\n",
    "        model.encoder.layers[_i].self_attn = new_attn\n",
    "    if args.can_print: print(f'lora clip_model, scale:{lora_scale} rate:{lora_rate} num:{lora_num}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35bb142c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:20:55.566558Z",
     "iopub.status.busy": "2023-05-23T07:20:55.565799Z",
     "iopub.status.idle": "2023-05-23T07:20:55.575092Z",
     "shell.execute_reply": "2023-05-23T07:20:55.574104Z"
    },
    "papermill": {
     "duration": 0.022379,
     "end_time": "2023-05-23T07:20:55.577033",
     "exception": false,
     "start_time": "2023-05-23T07:20:55.554654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseClipNet(torch.nn.Module):\n",
    "    def __init__(self, args, lora_rate=0.0, nfz_rate=0.0, lora_scale=1.0, rank=4):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        config = CLIPConfig.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH(args.model_name))\n",
    "        config.vision_config.image_size = args.image_size\n",
    "        clip = CLIPModel(config)\n",
    "\n",
    "        pretrained_file = f'{PRETRAINED_MODEL_NAME_OR_PATH(args.model_name)}/pytorch_model.pt'\n",
    "        load_pretrained(clip, pretrained_file, strict=False, can_print=True)\n",
    "\n",
    "        self.vision_model = clip.vision_model\n",
    "        fc_dim = 16 * 1024\n",
    "        ebd_dim = self.vision_model.embeddings.position_embedding.embedding_dim\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(ebd_dim, fc_dim),\n",
    "            nn.BatchNorm1d(fc_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_dim, args.embedding_dim),\n",
    "        )\n",
    "        lora_clip_model(self.vision_model, args, lora_scale, lora_rate, nfz_rate, rank)\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = self.vision_model(data['image'])\n",
    "        logits = self.head(out['pooler_output'])\n",
    "        return logits\n",
    "\n",
    "def clipnet_lora06(args):\n",
    "    model = BaseClipNet(args, lora_rate=0.6, nfz_rate=0.4)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f06361b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:20:55.599305Z",
     "iopub.status.busy": "2023-05-23T07:20:55.598575Z",
     "iopub.status.idle": "2023-05-23T07:21:31.764076Z",
     "shell.execute_reply": "2023-05-23T07:21:31.762672Z"
    },
    "papermill": {
     "duration": 36.1794,
     "end_time": "2023-05-23T07:21:31.766632",
     "exception": false,
     "start_time": "2023-05-23T07:20:55.587232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained file: /kaggle/input/diffusion2023-pretrained/clip-vit-large-patch14-336/pytorch_model.pt\n",
      "lora clip_model, scale:1.0 rate:0.6 num:14\n",
      "load pretrained file: /kaggle/working/result/models/train2m_pretrain6-6m_lora06_hd_openai_clip_vit_large_patch14_336_336x336_1folds/fold0/en3_ema.pth\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]dataset len: 7\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.19s/it]\n",
      "save pred to: /kaggle/working/result/submissions/train2m_pretrain6-6m_lora06_hd_openai_clip_vit_large_patch14_336_336x336_1folds/fold0/test_epochen3_ema_pred.csv\n",
      "     imgId_eId        val\n",
      "0  20057f34d_0   9.328125\n",
      "1  20057f34d_1  22.015625\n",
      "2  20057f34d_2   2.335938\n",
      "3  20057f34d_3   0.904297\n",
      "4  20057f34d_4   9.601562\n",
      "score: 0.69406\n"
     ]
    }
   ],
   "source": [
    "do_predict({\n",
    "    'out_dir': 'train2m_pretrain6-6m_lora06_hd_openai_clip_vit_large_patch14_336_336x336_1folds',\n",
    "    'model': 'clipnet_lora06',\n",
    "    'module': 'clip_net',\n",
    "    'model_name': 'openai/clip-vit-large-patch14-336',\n",
    "    'image_size': 336,\n",
    "    'epoch': 'en3_ema',\n",
    "    'dataset': 'test',\n",
    "    'gpus': '0',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119e8e3",
   "metadata": {
    "papermill": {
     "duration": 0.011596,
     "end_time": "2023-05-23T07:21:31.790199",
     "exception": false,
     "start_time": "2023-05-23T07:21:31.778603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b3a6936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:21:31.815372Z",
     "iopub.status.busy": "2023-05-23T07:21:31.815021Z",
     "iopub.status.idle": "2023-05-23T07:21:31.820673Z",
     "shell.execute_reply": "2023-05-23T07:21:31.819645Z"
    },
    "papermill": {
     "duration": 0.021065,
     "end_time": "2023-05-23T07:21:31.822954",
     "exception": false,
     "start_time": "2023-05-23T07:21:31.801889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "en_config = [\n",
    "    {\n",
    "        'out_dir': 'train2m_pretrain6-6m_lora06_hd_openai_clip_vit_large_patch14_336_336x336_1folds', \n",
    "        'weight': 0.25\n",
    "    }, \n",
    "    {\n",
    "        'out_dir': 'train2m_pretrain6-6m_lora04_blip2_opt_2_7b_224x224_1folds', \n",
    "        'weight': 0.35\n",
    "    }, \n",
    "    {\n",
    "        'out_dir': 'train2m_pretrain6-6m_lora04_laion_CLIP_convnext_xxlarge_laion2B_s34B_b82K_augreg_soup_256x256_1folds', \n",
    "        'weight': 0.4\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67490ac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:21:31.847884Z",
     "iopub.status.busy": "2023-05-23T07:21:31.847070Z",
     "iopub.status.idle": "2023-05-23T07:21:31.915547Z",
     "shell.execute_reply": "2023-05-23T07:21:31.914355Z"
    },
    "papermill": {
     "duration": 0.083213,
     "end_time": "2023-05-23T07:21:31.917657",
     "exception": false,
     "start_time": "2023-05-23T07:21:31.834444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.70161\n"
     ]
    }
   ],
   "source": [
    "dataset = 'test'\n",
    "meta_df = pd.read_csv(f'{DATA_DIR}/raw/sample_submission.csv')\n",
    "values_sum = 0.\n",
    "weight_sum = 0.\n",
    "for idx, cfg in enumerate(en_config):\n",
    "    out_dir = cfg['out_dir']\n",
    "    weight = cfg['weight']\n",
    "    fold = 0\n",
    "    epoch = 'en3_ema'\n",
    "    pred_path_suffix = f'{dataset}_epoch{epoch}'\n",
    "    pred_df = pd.read_csv(f\"{RESULT_DIR}/submissions/{out_dir}/fold{fold}/{pred_path_suffix}_pred.csv\")\n",
    "    values = meta_df[[ID]].merge(pred_df, on=ID, how='left')[TARGET].values\n",
    "    values = values.reshape(-1, EMBEDDING_LENGTH)\n",
    "    mean = values.mean(axis=1, keepdims=True)\n",
    "    std = values.std(axis=1, keepdims=True)\n",
    "    values = (values - mean) / (std + 1e-8)\n",
    "    values = values.reshape(-1)\n",
    "    values_sum += values * weight\n",
    "    weight_sum += weight\n",
    "values = values_sum / weight_sum\n",
    "pred_df = pd.DataFrame({\n",
    "    ID: meta_df[ID].values,\n",
    "    TARGET: values,\n",
    "})\n",
    "if not IS_KAGGLE_SUBMIT:\n",
    "    pred_embeddings = pred_df[TARGET].values.reshape(-1, EMBEDDING_LENGTH)\n",
    "    truth_embeddings = meta_df[TARGET].values.reshape(-1, EMBEDDING_LENGTH)\n",
    "    score = cosine_similarity(truth_embeddings, pred_embeddings)\n",
    "    print(f'score: {score:.5f}')\n",
    "pred_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "915d446f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:21:31.942841Z",
     "iopub.status.busy": "2023-05-23T07:21:31.942556Z",
     "iopub.status.idle": "2023-05-23T07:21:32.337213Z",
     "shell.execute_reply": "2023-05-23T07:21:32.336014Z"
    },
    "papermill": {
     "duration": 0.410287,
     "end_time": "2023-05-23T07:21:32.339961",
     "exception": false,
     "start_time": "2023-05-23T07:21:31.929674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir_list = os.listdir('/kaggle/working')\n",
    "for file in dir_list:\n",
    "    if os.path.isdir(file):\n",
    "        shutil.rmtree(file)\n",
    "    elif file != 'submission.csv':\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c9f16da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T07:21:32.366632Z",
     "iopub.status.busy": "2023-05-23T07:21:32.365113Z",
     "iopub.status.idle": "2023-05-23T07:21:32.378258Z",
     "shell.execute_reply": "2023-05-23T07:21:32.377051Z"
    },
    "papermill": {
     "duration": 0.028142,
     "end_time": "2023-05-23T07:21:32.380364",
     "exception": false,
     "start_time": "2023-05-23T07:21:32.352222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission: (2688, 2)\n",
      "     imgId_eId       val\n",
      "0  20057f34d_0  0.544083\n",
      "1  20057f34d_1  1.333054\n",
      "2  20057f34d_2  0.204143\n",
      "3  20057f34d_3  0.021722\n",
      "4  20057f34d_4  0.083885\n"
     ]
    }
   ],
   "source": [
    "submit_df = pd.read_csv('./submission.csv')\n",
    "print(f'submission: {submit_df.shape}')\n",
    "print(submit_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 494.970042,
   "end_time": "2023-05-23T07:21:35.259396",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-23T07:13:20.289354",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
